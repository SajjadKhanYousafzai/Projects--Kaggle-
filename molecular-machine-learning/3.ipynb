{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bb8bf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import streamlit as st\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.feature_selection import RFE, RFECV, SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBRegressor\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, AllChem, Lipinski, MolSurf, QED, rdMolDescriptors\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "# Custom color palette for visualizations\n",
    "colors = {\n",
    "    'primary': '#1f77b4',\n",
    "    'secondary': '#ff7f0e',\n",
    "    'tertiary': '#2ca02c',\n",
    "    'quaternary': '#d62728',\n",
    "    'highlight': '#9467bd'\n",
    "}\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16781279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate submission\n",
    "# submission = pd.DataFrame({'Batch_ID': df_test['Batch_ID'], 'T80': y_pred_ensemble})\n",
    "# submission.to_csv('submission21.csv', index=False)\n",
    "# print(\"Submission saved to submission2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3591c8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_path, test_path):\n",
    "    \"\"\"\n",
    "    Load training and test datasets\n",
    "    \"\"\"\n",
    "    df_train = pd.read_csv(train_path)\n",
    "    df_test = pd.read_csv(test_path)\n",
    "    submission = pd.read_csv(\"../molecular-machine-learning/data/sample_submission.csv\")\n",
    "    \n",
    "    print(f\"Training data shape: {df_train.shape}\")\n",
    "    print(f\"Test data shape: {df_test.shape}\")\n",
    "    \n",
    "    return df_train, df_test, submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e62041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for data exploration\n",
    "def explore_data(df):\n",
    "    \"\"\"\n",
    "    Perform initial data exploration\n",
    "    \"\"\"\n",
    "    print(\"\\n==== Data Overview ====\")\n",
    "    print(f\"Dataset contains {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "    \n",
    "    # Data types and missing values\n",
    "    data_types = pd.DataFrame({\n",
    "        'Type': df.dtypes,\n",
    "        'Non-Null Count': df.count(),\n",
    "        'Missing Values': df.isnull().sum(),\n",
    "        'Missing Percentage': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "    })\n",
    "    \n",
    "    print(\"\\n==== Data Types and Missing Values ====\")\n",
    "    print(data_types[data_types['Missing Values'] > 0])\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"\\n==== Numerical Features Statistics ====\")\n",
    "    print(df.describe().T)\n",
    "    \n",
    "    return data_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eb4224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for data cleaning\n",
    "def clean_data(df_train, df_test):\n",
    "    \"\"\"\n",
    "    Clean the datasets by handling missing values and outliers\n",
    "    \"\"\"\n",
    "    print(\"\\n==== Data Cleaning ====\")\n",
    "    \n",
    "    # Create copies to avoid modifying original data\n",
    "    train_clean = df_train.copy()\n",
    "    test_clean = df_test.copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    for column in train_clean.select_dtypes(include=[np.number]).columns:\n",
    "        if train_clean[column].isnull().any():\n",
    "            median_val = train_clean[column].median()\n",
    "            train_clean[column] = train_clean[column].fillna(median_val)\n",
    "            test_clean[column] = test_clean[column].fillna(median_val)\n",
    "            print(f\"Filled missing values in {column} with median: {median_val}\")\n",
    "    \n",
    "    # Handle outliers for key features\n",
    "    key_features = ['T80', 'Mass', 'NumHeteroatoms', 'TDOS4.0']\n",
    "    for feature in key_features:\n",
    "        if feature in train_clean.columns:\n",
    "            Q1 = train_clean[feature].quantile(0.25)\n",
    "            Q3 = train_clean[feature].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 3 * IQR  # Less aggressive outlier removal\n",
    "            upper_bound = Q3 + 3 * IQR\n",
    "            \n",
    "            # Count outliers\n",
    "            outliers_count = ((train_clean[feature] < lower_bound) | (train_clean[feature] > upper_bound)).sum()\n",
    "            \n",
    "            if outliers_count > 0:\n",
    "                print(f\"Capping {outliers_count} outliers in {feature}\")\n",
    "                train_clean[feature] = train_clean[feature].clip(lower_bound, upper_bound)\n",
    "                if feature != 'T80':  # Don't cap T80 in test set as it's the target\n",
    "                    test_clean[feature] = test_clean[feature].clip(lower_bound, upper_bound)\n",
    "    \n",
    "    return train_clean, test_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66412c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for feature engineering using RDKit\n",
    "def engineer_rdkit_features(df_train, df_test):\n",
    "    \"\"\"\n",
    "    Engineer additional molecular features using RDKit\n",
    "    \"\"\"\n",
    "    print(\"\\n==== RDKit Feature Engineering ====\")\n",
    "    \n",
    "    def compute_rdkit_features(smiles):\n",
    "        \"\"\"Calculate RDKit molecular descriptors from SMILES string\"\"\"\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                return None\n",
    "            \n",
    "            features = {\n",
    "                # Basic properties\n",
    "                'RDKit_MolWt': Descriptors.MolWt(mol),\n",
    "                'RDKit_LogP': Descriptors.MolLogP(mol),\n",
    "                'RDKit_TPSA': Descriptors.TPSA(mol),\n",
    "                \n",
    "                # Electronic properties\n",
    "                'RDKit_NumValenceElectrons': Descriptors.NumValenceElectrons(mol),\n",
    "                'RDKit_NumRadicalElectrons': Descriptors.NumRadicalElectrons(mol),\n",
    "                \n",
    "                # Lipinski properties\n",
    "                'RDKit_HBD': Lipinski.NumHDonors(mol),\n",
    "                'RDKit_HBA': Lipinski.NumHAcceptors(mol),\n",
    "                'RDKit_RotBonds': Lipinski.NumRotatableBonds(mol),\n",
    "                \n",
    "                # Topological properties\n",
    "                'RDKit_Rings': rdMolDescriptors.CalcNumRings(mol),\n",
    "                'RDKit_AromaticRings': rdMolDescriptors.CalcNumAromaticRings(mol),\n",
    "                'RDKit_HeteroRings': rdMolDescriptors.CalcNumHeteroRings(mol),\n",
    "                'RDKit_Sp3Atoms': rdMolDescriptors.CalcFractionCSP3(mol),\n",
    "                \n",
    "                # Surface and volume properties\n",
    "                'RDKit_LabuteASA': rdMolDescriptors.CalcLabuteASA(mol),\n",
    "                'RDKit_PEOE_VSA': sum(rdMolDescriptors.PEOE_VSA_(mol)),\n",
    "                'RDKit_SMR_VSA': sum(rdMolDescriptors.SMR_VSA_(mol)),\n",
    "                \n",
    "                # QED and related\n",
    "                'RDKit_QED': QED.qed(mol),\n",
    "                \n",
    "                # Morgan fingerprints (ECFP) - aggregated properties\n",
    "                'RDKit_ECFP4_Count': len(AllChem.GetMorganFingerprint(mol, 2).GetNonzeroElements()),\n",
    "                'RDKit_ECFP4_Complexity': sum(AllChem.GetMorganFingerprint(mol, 2).GetNonzeroElements().values()),\n",
    "                \n",
    "                # Additional electronic properties\n",
    "                'RDKit_PEOE_VSA_PosSum': sum([x for x in rdMolDescriptors.PEOE_VSA_(mol) if x > 0]),\n",
    "                'RDKit_PEOE_VSA_NegSum': sum([x for x in rdMolDescriptors.PEOE_VSA_(mol) if x < 0]),\n",
    "            }\n",
    "            \n",
    "            return pd.Series(features)\n",
    "        except:\n",
    "            print(f\"Error processing SMILES: {smiles}\")\n",
    "            return None\n",
    "    \n",
    "    # Apply feature engineering to both datasets\n",
    "    train_rdkit = df_train['Smiles'].apply(compute_rdkit_features)\n",
    "    test_rdkit = df_test['Smiles'].apply(compute_rdkit_features)\n",
    "    \n",
    "    # Check for invalid SMILES\n",
    "    invalid_train = train_rdkit.isna().all(axis=1)\n",
    "    invalid_test = test_rdkit.isna().all(axis=1)\n",
    "    \n",
    "    if invalid_train.any():\n",
    "        print(f\"Found {invalid_train.sum()} invalid SMILES in training data, removing them\")\n",
    "        valid_train_idx = ~invalid_train\n",
    "        df_train = df_train.loc[valid_train_idx].reset_index(drop=True)\n",
    "        train_rdkit = train_rdkit.loc[valid_train_idx].reset_index(drop=True)\n",
    "    \n",
    "    if invalid_test.any():\n",
    "        print(f\"Found {invalid_test.sum()} invalid SMILES in test data, removing them\")\n",
    "        valid_test_idx = ~invalid_test\n",
    "        df_test = df_test.loc[valid_test_idx].reset_index(drop=True)\n",
    "        test_rdkit = test_rdkit.loc[valid_test_idx].reset_index(drop=True)\n",
    "    \n",
    "    # Merge RDKit features with original data\n",
    "    df_train_enhanced = pd.concat([df_train, train_rdkit], axis=1)\n",
    "    df_test_enhanced = pd.concat([df_test, test_rdkit], axis=1)\n",
    "    \n",
    "    print(f\"Added {train_rdkit.shape[1]} RDKit features\")\n",
    "    \n",
    "    return df_train_enhanced, df_test_enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f48a3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for data visualization\n",
    "def visualize_data(df_train):\n",
    "    \"\"\"\n",
    "    Create visualizations for data analysis\n",
    "    \"\"\"\n",
    "    print(\"\\n==== Data Visualization ====\")\n",
    "    \n",
    "    # 1. Target variable distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df_train['T80'], kde=True, color=colors['primary'])\n",
    "    plt.title('Distribution of T80 (Photostability Lifetime)', fontsize=14)\n",
    "    plt.xlabel('T80 Value', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('t80_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # 2. Correlation between key features and target\n",
    "    key_features = ['TDOS4.0', 'NumHeteroatoms', 'Mass']\n",
    "    plt.figure(figsize=(18, 5))\n",
    "    \n",
    "    for i, feature in enumerate(key_features, 1):\n",
    "        plt.subplot(1, 3, i)\n",
    "        sns.scatterplot(x=df_train[feature], y=df_train['T80'], color=colors['secondary'], alpha=0.7)\n",
    "        \n",
    "        # Add regression line\n",
    "        sns.regplot(x=df_train[feature], y=df_train['T80'], scatter=False, color=colors['quaternary'])\n",
    "        \n",
    "        plt.title(f'{feature} vs T80', fontsize=12)\n",
    "        plt.xlabel(feature, fontsize=10)\n",
    "        plt.ylabel('T80', fontsize=10)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('key_features_correlation.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # 3. Correlation heatmap\n",
    "    # Select numerical columns excluding ID columns and SMILES\n",
    "    numerical_cols = df_train.select_dtypes(include=[np.number]).columns\n",
    "    numerical_cols = [col for col in numerical_cols if 'ID' not in col]\n",
    "    \n",
    "    # Calculate correlations\n",
    "    corr_matrix = df_train[numerical_cols].corr()\n",
    "    \n",
    "    # Focus on correlations with T80\n",
    "    t80_corr = corr_matrix['T80'].sort_values(ascending=False)\n",
    "    \n",
    "    # Top 15 correlations with T80\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        corr_matrix.loc[t80_corr.index[:15], t80_corr.index[:15]], \n",
    "        annot=True, \n",
    "        cmap='viridis', \n",
    "        linewidths=0.5, \n",
    "        fmt=\".2f\",\n",
    "        annot_kws={\"size\": 8}\n",
    "    )\n",
    "    plt.title('Correlation Heatmap: Top 15 Features by T80 Correlation', fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # 4. PCA Visualization (2D projection of molecules)\n",
    "    # Select numerical features\n",
    "    feature_cols = [col for col in df_train.select_dtypes(include=[np.number]).columns \n",
    "                   if col != 'T80' and 'ID' not in col]\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(df_train[feature_cols])\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(scaled_features)\n",
    "    \n",
    "    # Create DataFrame for plotting\n",
    "    pca_df = pd.DataFrame({\n",
    "        'PC1': pca_result[:, 0],\n",
    "        'PC2': pca_result[:, 1],\n",
    "        'T80': df_train['T80']\n",
    "    })\n",
    "    \n",
    "    # Plot PCA results\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(\n",
    "        pca_df['PC1'], \n",
    "        pca_df['PC2'], \n",
    "        c=pca_df['T80'], \n",
    "        cmap='viridis',\n",
    "        alpha=0.7,\n",
    "        s=50\n",
    "    )\n",
    "    plt.colorbar(scatter, label='T80 Value')\n",
    "    plt.title('PCA: 2D Projection of Molecules Colored by T80', fontsize=14)\n",
    "    plt.xlabel(f'Principal Component 1 ({pca.explained_variance_ratio_[0]:.2%} variance)', fontsize=12)\n",
    "    plt.ylabel(f'Principal Component 2 ({pca.explained_variance_ratio_[1]:.2%} variance)', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('pca_visualization.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # 5. Pairplot of key features\n",
    "    key_features_with_target = key_features + ['T80', 'RDKit_TPSA', 'RDKit_QED'] \n",
    "    key_features_with_target = [f for f in key_features_with_target if f in df_train.columns]\n",
    "    \n",
    "    plt.figure(figsize=(15, 12))\n",
    "    sns.pairplot(\n",
    "        df_train[key_features_with_target], \n",
    "        diag_kind='kde',\n",
    "        plot_kws={'alpha': 0.6, 'color': colors['primary']},\n",
    "        diag_kws={'color': colors['secondary']}\n",
    "    )\n",
    "    plt.suptitle('Pairplot of Key Features', y=1.02, fontsize=16)\n",
    "    plt.savefig('key_features_pairplot.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    print(\"Visualizations saved as PNG files\")\n",
    "    \n",
    "    return {\n",
    "        't80_distribution.png': 'Distribution of target variable',\n",
    "        'key_features_correlation.png': 'Correlation between key features and target',\n",
    "        'correlation_heatmap.png': 'Correlation heatmap of top features',\n",
    "        'pca_visualization.png': 'PCA visualization of molecules',\n",
    "        'key_features_pairplot.png': 'Pairplot of key features'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6c0856b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for feature selection\n",
    "def select_features(X_train, y_train, X_test, method='combined'):\n",
    "    \"\"\"\n",
    "    Select important features using multiple approaches\n",
    "    \"\"\"\n",
    "    print(\"\\n==== Feature Selection ====\")\n",
    "    \n",
    "    # Ensure key features are included\n",
    "    key_features = ['TDOS4.0', 'NumHeteroatoms', 'Mass']\n",
    "    key_features_present = [f for f in key_features if f in X_train.columns]\n",
    "    \n",
    "    if method == 'random_forest':\n",
    "        # Random Forest feature importance\n",
    "        rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        \n",
    "        # Get feature importances\n",
    "        importances = rf.feature_importances_\n",
    "        feature_importance = pd.DataFrame({'Feature': X_train.columns, 'Importance': importances})\n",
    "        feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Select top features\n",
    "        top_features = feature_importance.head(20)['Feature'].tolist()\n",
    "        \n",
    "        # Make sure key features are included\n",
    "        for feature in key_features_present:\n",
    "            if feature not in top_features:\n",
    "                top_features.append(feature)\n",
    "                \n",
    "        print(f\"Selected {len(top_features)} features using Random Forest Importance\")\n",
    "        \n",
    "    elif method == 'rfe':\n",
    "        # Recursive Feature Elimination\n",
    "        estimator = SVR(kernel='linear')\n",
    "        selector = RFECV(estimator, step=1, cv=5, min_features_to_select=10, scoring='neg_mean_squared_error')\n",
    "        selector.fit(X_train, y_train)\n",
    "        \n",
    "        # Get selected features\n",
    "        selected_features_mask = selector.support_\n",
    "        top_features = X_train.columns[selected_features_mask].tolist()\n",
    "        \n",
    "        # Make sure key features are included\n",
    "        for feature in key_features_present:\n",
    "            if feature not in top_features:\n",
    "                top_features.append(feature)\n",
    "                \n",
    "        print(f\"Selected {len(top_features)} features using RFE with cross-validation\")\n",
    "        \n",
    "    elif method == 'combined':\n",
    "        # Combine multiple feature selection methods\n",
    "        \n",
    "        # Method 1: Random Forest Importance\n",
    "        rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        \n",
    "        rf_importance = pd.DataFrame({\n",
    "            'Feature': X_train.columns,\n",
    "            'RF_Importance': rf.feature_importances_\n",
    "        }).sort_values('RF_Importance', ascending=False)\n",
    "        \n",
    "        # Method 2: Lasso Coefficient\n",
    "        lasso = Lasso(alpha=0.01, random_state=42)\n",
    "        lasso.fit(X_train, y_train)\n",
    "        \n",
    "        lasso_importance = pd.DataFrame({\n",
    "            'Feature': X_train.columns,\n",
    "            'Lasso_Coef': np.abs(lasso.coef_)\n",
    "        }).sort_values('Lasso_Coef', ascending=False)\n",
    "        \n",
    "        # Method 3: Correlation with target\n",
    "        corr_with_target = pd.DataFrame({\n",
    "            'Feature': X_train.columns,\n",
    "            'Correlation': [abs(np.corrcoef(X_train[col], y_train)[0, 1]) for col in X_train.columns]\n",
    "        }).sort_values('Correlation', ascending=False)\n",
    "        \n",
    "        # Combine rankings\n",
    "        feature_ranks = pd.DataFrame({'Feature': X_train.columns})\n",
    "        \n",
    "        # Add rankings from each method\n",
    "        feature_ranks = feature_ranks.merge(\n",
    "            rf_importance[['Feature', 'RF_Importance']], \n",
    "            on='Feature', \n",
    "            how='left'\n",
    "        ).merge(\n",
    "            lasso_importance[['Feature', 'Lasso_Coef']], \n",
    "            on='Feature', \n",
    "            how='left'\n",
    "        ).merge(\n",
    "            corr_with_target[['Feature', 'Correlation']], \n",
    "            on='Feature', \n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Create a combined score (normalized sum of all methods)\n",
    "        for col in ['RF_Importance', 'Lasso_Coef', 'Correlation']:\n",
    "            feature_ranks[f'{col}_Norm'] = feature_ranks[col] / feature_ranks[col].max()\n",
    "        \n",
    "        feature_ranks['Combined_Score'] = (\n",
    "            feature_ranks['RF_Importance_Norm'] + \n",
    "            feature_ranks['Lasso_Coef_Norm'] + \n",
    "            feature_ranks['Correlation_Norm']\n",
    "        )\n",
    "        \n",
    "        # Sort by combined score\n",
    "        feature_ranks = feature_ranks.sort_values('Combined_Score', ascending=False)\n",
    "        \n",
    "        # Select top features\n",
    "        top_features = feature_ranks.head(20)['Feature'].tolist()\n",
    "        \n",
    "        # Make sure key features are included\n",
    "        for feature in key_features_present:\n",
    "            if feature not in top_features:\n",
    "                top_features.append(feature)\n",
    "                \n",
    "        print(f\"Selected {len(top_features)} features using combined importance methods\")\n",
    "        \n",
    "        # Display top 10 features with scores\n",
    "        print(\"\\nTop 10 features by combined importance:\")\n",
    "        top_10_display = feature_ranks.head(10)[['Feature', 'Combined_Score', 'RF_Importance', 'Lasso_Coef', 'Correlation']]\n",
    "        print(top_10_display)\n",
    "    \n",
    "    else:\n",
    "        # If no method specified, use all features\n",
    "        top_features = X_train.columns.tolist()\n",
    "        print(f\"Using all {len(top_features)} features\")\n",
    "    \n",
    "    # Filter datasets to selected features\n",
    "    X_train_selected = X_train[top_features]\n",
    "    X_test_selected = X_test[top_features]\n",
    "    \n",
    "    return X_train_selected, X_test_selected, top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1408bd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for model evaluation\n",
    "def evaluate_models(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Evaluate multiple regression models\n",
    "    \"\"\"\n",
    "    print(\"\\n==== Model Evaluation ====\")\n",
    "    \n",
    "    # Define models to evaluate\n",
    "    models = {\n",
    "        'SVR': SVR(),\n",
    "        'RandomForest': RandomForestRegressor(random_state=42),\n",
    "        'GradientBoosting': GradientBoostingRegressor(random_state=42),\n",
    "        'XGBoost': XGBRegressor(random_state=42),\n",
    "        'Lasso': Lasso(random_state=42),\n",
    "        'Ridge': Ridge(random_state=42),\n",
    "        'ElasticNet': ElasticNet(random_state=42)\n",
    "    }\n",
    "    \n",
    "    # Cross-validation settings\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    results = {}\n",
    "    \n",
    "    # Evaluate each model\n",
    "    for name, model in models.items():\n",
    "        # Calculate cross-validation scores\n",
    "        rmse_scores = np.sqrt(-cross_val_score(model, X_train, y_train, \n",
    "                                             scoring='neg_mean_squared_error', \n",
    "                                             cv=cv))\n",
    "        r2_scores = cross_val_score(model, X_train, y_train, \n",
    "                                    scoring='r2', \n",
    "                                    cv=cv)\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'RMSE': rmse_scores.mean(),\n",
    "            'RMSE_std': rmse_scores.std(),\n",
    "            'R2': r2_scores.mean(),\n",
    "            'R2_std': r2_scores.std()\n",
    "        }\n",
    "        \n",
    "        print(f\"{name}: RMSE = {rmse_scores.mean():.4f} ± {rmse_scores.std():.4f}, R² = {r2_scores.mean():.4f} ± {r2_scores.std():.4f}\")\n",
    "    \n",
    "    # Identify best model\n",
    "    best_model = min(results.items(), key=lambda x: x[1]['RMSE'])[0]\n",
    "    print(f\"\\nBest model based on RMSE: {best_model}\")\n",
    "    \n",
    "    return results, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46d28808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for hyperparameter tuning\n",
    "def tune_model(X_train, y_train, model_name):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning for the selected model\n",
    "    \"\"\"\n",
    "    print(f\"\\n==== Hyperparameter Tuning for {model_name} ====\")\n",
    "    \n",
    "    if model_name == 'SVR':\n",
    "        # Define search space for SVR\n",
    "        param_grid = {\n",
    "            'C': [0.1, 1, 10, 50, 100],\n",
    "            'epsilon': [0.001, 0.01, 0.1, 0.2],\n",
    "            'kernel': ['linear', 'rbf', 'poly'],\n",
    "            'gamma': ['scale', 'auto', 0.1, 0.01]\n",
    "        }\n",
    "        base_model = SVR()\n",
    "        \n",
    "    elif model_name == 'RandomForest':\n",
    "        # Define search space for Random Forest\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'max_features': ['sqrt', 'log2', None]\n",
    "        }\n",
    "        base_model = RandomForestRegressor(random_state=42)\n",
    "        \n",
    "    elif model_name == 'GradientBoosting':\n",
    "        # Define search space for Gradient Boosting\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'subsample': [0.7, 0.8, 0.9, 1.0]\n",
    "        }\n",
    "        base_model = GradientBoostingRegressor(random_state=42)\n",
    "        \n",
    "    elif model_name == 'XGBoost':\n",
    "        # Define search space for XGBoost\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'min_child_weight': [1, 3, 5],\n",
    "            'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "            'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "            'gamma': [0, 0.1, 0.2]\n",
    "        }\n",
    "        base_model = XGBRegressor(random_state=42)\n",
    "        \n",
    "    elif model_name == 'Lasso':\n",
    "        # Define search space for Lasso\n",
    "        param_grid = {\n",
    "            'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
    "            'selection': ['cyclic', 'random'],\n",
    "            'tol': [1e-4, 1e-3, 1e-2]\n",
    "        }\n",
    "        base_model = Lasso(random_state=42)\n",
    "        \n",
    "    elif model_name == 'Ridge':\n",
    "        # Define search space for Ridge\n",
    "        param_grid = {\n",
    "            'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "            'solver': ['auto', 'svd', 'cholesky', 'lsqr'],\n",
    "        }\n",
    "        base_model = Ridge(random_state=42)\n",
    "        \n",
    "    elif model_name == 'ElasticNet':\n",
    "        # Define search space for ElasticNet\n",
    "        param_grid = {\n",
    "            'alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "            'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "            'selection': ['cyclic', 'random']\n",
    "        }\n",
    "        base_model = ElasticNet(random_state=42)\n",
    "        \n",
    "    else:\n",
    "        print(f\"Model {model_name} not supported for tuning\")\n",
    "        return None\n",
    "    \n",
    "    # Use RandomizedSearchCV for faster tuning with cross-validation\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=base_model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=50,  # Number of parameter settings sampled\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Fit to data\n",
    "    search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get best parameters and score\n",
    "    best_params = search.best_params_\n",
    "    best_score = np.sqrt(-search.best_score_)\n",
    "    \n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "    print(f\"Best RMSE: {best_score:.4f}\")\n",
    "    \n",
    "    # Create model with best parameters\n",
    "    if model_name == 'SVR':\n",
    "        best_model = SVR(**best_params)\n",
    "    elif model_name == 'RandomForest':\n",
    "        best_model = RandomForestRegressor(random_state=42, **best_params)\n",
    "    elif model_name == 'GradientBoosting':\n",
    "        best_model = GradientBoostingRegressor(random_state=42, **best_params)\n",
    "    elif model_name == 'XGBoost':\n",
    "        best_model = XGBRegressor(random_state=42, **best_params)\n",
    "    elif model_name == 'Lasso':\n",
    "        best_model = Lasso(random_state=42, **best_params)\n",
    "    elif model_name == 'Ridge':\n",
    "        best_model = Ridge(random_state=42, **best_params)\n",
    "    elif model_name == 'ElasticNet':\n",
    "        best_model = ElasticNet(random_state=42, **best_params)\n",
    "    \n",
    "    return best_model, best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91b8c6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create ensemble model\n",
    "def create_ensemble(X_train, y_train, models_dict):\n",
    "    \"\"\"\n",
    "    Create an ensemble of the best models\n",
    "    \"\"\"\n",
    "    print(\"\\n==== Creating Ensemble Model ====\")\n",
    "    \n",
    "    # Select models for ensemble\n",
    "    ensemble_models = {}\n",
    "    for model_name, model_obj in models_dict.items():\n",
    "        ensemble_models[model_name] = model_obj\n",
    "        \n",
    "    # Train all models\n",
    "    predictions = {}\n",
    "    for name, model in ensemble_models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions[name] = model.predict(X_train)\n",
    "    \n",
    "    # Calculate optimal weights using a simple meta-model\n",
    "    # Convert predictions to DataFrame\n",
    "    pred_df = pd.DataFrame(predictions)\n",
    "    \n",
    "    # Meta-model: Ridge regression to find optimal weights\n",
    "    meta_model = Ridge(alpha=1.0)\n",
    "    meta_model.fit(pred_df, y_train)\n",
    "    \n",
    "    # Get weights (coefficients)\n",
    "    weights = meta_model.coef_\n",
    "    weights = np.maximum(weights, 0)  # Ensure non-negative weights\n",
    "    weights = weights / weights.sum()  # Normalize weights to sum to 1\n",
    "    \n",
    "    # Create weights dictionary\n",
    "    weights_dict = {model_name: weight for model_name, weight in zip(ensemble_models.keys(), weights)}\n",
    "    \n",
    "    print(\"Ensemble model weights:\")\n",
    "    for model_name, weight in weights_dict.items():\n",
    "        print(f\"  {model_name}: {weight:.4f}\")\n",
    "    \n",
    "    # Create ensemble prediction function\n",
    "    def ensemble_predict(X):\n",
    "        \"\"\"Generate predictions using the weighted ensemble\"\"\"\n",
    "        pred = np.zeros(X.shape[0])\n",
    "        for name, model in ensemble_models.items():\n",
    "            pred += weights_dict[name] * model.predict(X)\n",
    "        return pred\n",
    "    \n",
    "    # Test ensemble on training data\n",
    "    ensemble_train_pred = ensemble_predict(X_train)\n",
    "    ensemble_rmse = np.sqrt(mean_squared_error(y_train, ensemble_train_pred))\n",
    "    ensemble_r2 = r2_score(y_train, ensemble_train_pred)\n",
    "    \n",
    "    print(f\"Ensemble Training RMSE: {ensemble_rmse:.4f}\")\n",
    "    print(f\"Ensemble Training R²: {ensemble_r2:.4f}\")\n",
    "    \n",
    "    # Return ensemble components\n",
    "    ensemble = {\n",
    "        'models': ensemble_models,\n",
    "        'weights': weights_dict,\n",
    "        'predict': ensemble_predict,\n",
    "        'meta_model': meta_model,\n",
    "        'performance': {\n",
    "            'rmse': ensemble_rmse,\n",
    "            'r2': ensemble_r2\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a806d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions on test data\n",
    "def make_predictions(models, X_test, ensemble=None):\n",
    "    \"\"\"\n",
    "    Generate predictions on test data using individual models and ensemble\n",
    "    \"\"\"\n",
    "    print(\"\\n==== Making Predictions ====\")\n",
    "    \n",
    "    # Dictionary to store predictions\n",
    "    predictions = {}\n",
    "    \n",
    "    # Get predictions from individual models\n",
    "    for name, model in models.items():\n",
    "        predictions[name] = model.predict(X_test)\n",
    "    \n",
    "    # Add ensemble predictions if available\n",
    "    if ensemble:\n",
    "        predictions['Ensemble'] = ensemble['predict'](X_test)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    predictions_df = pd.DataFrame(predictions)\n",
    "    \n",
    "    return predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5cd5cb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize model results\n",
    "def visualize_model_results(results, y_train, train_predictions, top_features):\n",
    "    \"\"\"\n",
    "    Create visualizations of model performance and feature importance\n",
    "    \"\"\"\n",
    "    print(\"\\n==== Visualizing Model Results ====\")\n",
    "    \n",
    "    # 1. Model Performance Comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Extract RMSE values and create error bars\n",
    "    models = list(results.keys())\n",
    "    rmse_values = [results[model]['RMSE'] for model in models]\n",
    "    rmse_errors = [results[model]['RMSE_std'] for model in models]\n",
    "    \n",
    "    # Sort by performance\n",
    "    sorted_indices = np.argsort(rmse_values)\n",
    "    sorted_models = [models[i] for i in sorted_indices]\n",
    "    sorted_rmse = [rmse_values[i] for i in sorted_indices]\n",
    "    sorted_errors = [rmse_errors[i] for i in sorted_indices]\n",
    "    \n",
    "    # Create bar plot\n",
    "    bars = plt.bar(sorted_models, sorted_rmse, yerr=sorted_errors, \n",
    "                   color=colors['primary'], alpha=0.7, \n",
    "                   capsize=5, error_kw={'ecolor': colors['quaternary']})\n",
    "    \n",
    "    # Highlight best model\n",
    "    best_model_idx = sorted_models.index(min(results.items(), key=lambda x: x[1]['RMSE'])[0])\n",
    "    bars[best_model_idx].set_color(colors['secondary'])\n",
    "    \n",
    "    plt.title('Model Performance Comparison (RMSE)', fontsize=14)\n",
    "    plt.xlabel('Models', fontsize=12)\n",
    "    plt.ylabel('RMSE (Root Mean Squared Error)', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_performance.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # 2. Actual vs Predicted for best model\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Get predictions from best model\n",
    "    best_model_name = min(results.items(), key=lambda x: x[1]['RMSE'])[0]\n",
    "    best_model_preds = train_predictions[best_model_name]\n",
    "    \n",
    "    # Scatterplot of actual vs predicted\n",
    "    plt.scatter(y_train, best_model_preds, \n",
    "               color=colors['primary'], alpha=0.7, edgecolor='k', linewidth=0.5)\n",
    "    \n",
    "    # Add diagonal line (perfect predictions)\n",
    "    min_val = min(min(y_train), min(best_model_preds))\n",
    "    max_val = max(max(y_train), max(best_model_preds))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.5)\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.title(f'Actual vs Predicted T80 Values ({best_model_name})', fontsize=14)\n",
    "    plt.xlabel('Actual T80', fontsize=12)\n",
    "    plt.ylabel('Predicted T80', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add R² value\n",
    "    r2 = r2_score(y_train, best_model_preds)\n",
    "    plt.annotate(f'R² = {r2:.4f}', \n",
    "                xy=(0.05, 0.95), xycoords='axes fraction',\n",
    "                fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('actual_vs_predicted.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # 3. Feature Importance (for Random Forest or XGBoost model)\n",
    "    if 'RandomForest' in results or 'XGBoost' in results:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # Get feature importance from tree-based model\n",
    "        if 'RandomForest' in results:\n",
    "            model_name = 'RandomForest'\n",
    "        else:\n",
    "            model_name = 'XGBoost'\n",
    "            \n",
    "        # Get model\n",
    "        for name, model in train_predictions.items():\n",
    "            if name == model_name:\n",
    "                tree_model = model\n",
    "                break\n",
    "        \n",
    "        # Get feature importances\n",
    "        importances = tree_model.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "        # Plot top 15 features\n",
    "        plt.barh(range(min(15, len(top_features))), \n",
    "                importances[indices[:15]], \n",
    "                color=colors['tertiary'], alpha=0.7)\n",
    "        plt.yticks(range(min(15, len(top_features))), [top_features[i] for i in indices[:15]])\n",
    "        plt.title(f'Feature Importance ({model_name})', fontsize=14)\n",
    "        plt.xlabel('Importance', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    print(\"Model result visualizations saved as PNG files\")\n",
    "    return {\n",
    "        'model_performance.png': 'Comparison of model performance (RMSE)',\n",
    "        'actual_vs_predicted.png': 'Actual vs Predicted values for best model',\n",
    "        'feature_importance.png': 'Feature importance from tree-based model'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77bc1f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and save final model\n",
    "def train_final_model(X_train, y_train, best_model_type, best_params):\n",
    "    \"\"\"\n",
    "    Train the final model with the best parameters and save it\n",
    "    \"\"\"\n",
    "    print(\"\\n==== Training Final Model ====\")\n",
    "    \n",
    "    # Create model with best parameters\n",
    "    if best_model_type == 'SVR':\n",
    "        final_model = SVR(**best_params)\n",
    "    elif best_model_type == 'RandomForest':\n",
    "        final_model = RandomForestRegressor(random_state=42, **best_params)\n",
    "    elif best_model_type == 'GradientBoosting':\n",
    "        final_model = GradientBoostingRegressor(random_state=42, **best_params)\n",
    "    elif best_model_type == 'XGBoost':\n",
    "        final_model = XGBRegressor(random_state=42, **best_params)\n",
    "    elif best_model_type == 'Lasso':\n",
    "        final_model = Lasso(random_state=42, **best_params)\n",
    "    elif best_model_type == 'Ridge':\n",
    "        final_model = Ridge(random_state=42, **best_params)\n",
    "    elif best_model_type == 'ElasticNet':\n",
    "        final_model = ElasticNet(random_state=42, **best_params)\n",
    "    else:\n",
    "        print(f\"Model type {best_model_type} not supported\")\n",
    "        return None\n",
    "    \n",
    "    # Train model on full data\n",
    "    final_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Save model\n",
    "    with open('final_model.pkl', 'wb') as f:\n",
    "        pickle.dump(final_model, f)\n",
    "    \n",
    "    print(f\"Final {best_model_type} model trained and saved as 'final_model.pkl'\")\n",
    "    \n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ebae67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save project components\n",
    "def save_project_components(scaler, selected_features, feature_engineering_info, best_model_type, best_params):\n",
    "    \"\"\"\n",
    "    Save all necessary components for model deployment\n",
    "    \"\"\"\n",
    "    print(\"\\n==== Saving Project Components ====\")\n",
    "    \n",
    "    components = {\n",
    "        'scaler': scaler,\n",
    "        'selected_features': selected_features,\n",
    "        'feature_engineering_info': feature_engineering_info,\n",
    "        'best_model_type': best_model_type,\n",
    "        'best_params': best_params,\n",
    "        'model_date': pd.Timestamp.now().strftime('%Y-%m-%d')\n",
    "    }\n",
    "    \n",
    "    with open('model_components.pkl', 'wb') as f:\n",
    "        pickle.dump(components, f)\n",
    "    \n",
    "    print(\"Project components saved as 'model_components.pkl'\")\n",
    "    \n",
    "    return components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32623ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to run the whole pipeline\n",
    "def main_pipeline(train_path=\"../molecular-machine-learning/data/train.csv\", \n",
    "                 test_path=\"../molecular-machine-learning/data/test.csv\",\n",
    "                 submission_path=\"../molecular-machine-learning/data/sample_submission.csv\"):\n",
    "    \"\"\"\n",
    "    End-to-end pipeline for molecular photostability prediction\n",
    "    \"\"\"\n",
    "    print(\"\\n==== MOLECULAR PHOTOSTABILITY PREDICTION PIPELINE ====\")\n",
    "    \n",
    "    # 1. Load data\n",
    "    df_train, df_test, submission = load_data(train_path, test_path)\n",
    "    \n",
    "    # 2. Explore data\n",
    "    data_info = explore_data(df_train)\n",
    "    \n",
    "    # 3. Clean data\n",
    "    df_train_clean, df_test_clean = clean_data(df_train, df_test)\n",
    "    \n",
    "    # 4. Engineer features\n",
    "    df_train_enhanced, df_test_enhanced = engineer_rdkit_features(df_train_clean, df_test_clean)\n",
    "    \n",
    "    # 5. Create visualizations\n",
    "    visualizations = visualize_data(df_train_enhanced)\n",
    "    \n",
    "    # 6. Prepare data for modeling\n",
    "    # Split features and target\n",
    "    X_train = df_train_enhanced.drop(['T80', 'Batch_ID', 'Smiles'], axis=1, errors='ignore')\n",
    "    y_train = df_train_enhanced['T80']\n",
    "    X_test = df_test_enhanced.drop(['Batch_ID', 'Smiles'], axis=1, errors='ignore')\n",
    "    \n",
    "    # 7. Select features\n",
    "    X_train_selected, X_test_selected, selected_features = select_features(X_train, y_train, X_test, method='combined')\n",
    "    \n",
    "    # 8. Scale features\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "    X_test_scaled = scaler.transform(X_test_selected)\n",
    "    \n",
    "    # Convert back to DataFrame to keep feature names\n",
    "    X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train_selected.columns)\n",
    "    X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test_selected.columns)\n",
    "    \n",
    "    # 9. Evaluate models\n",
    "    model_results, best_model_type = evaluate_models(X_train_scaled_df, y_train)\n",
    "    \n",
    "    # 10. Tune best model\n",
    "    best_model, best_params, best_score = tune_model(X_train_scaled_df, y_train, best_model_type)\n",
    "    \n",
    "    # 11. Train individual models with best parameters\n",
    "    models = {}\n",
    "    \n",
    "    # Best SVR model\n",
    "    svr_params = {'C': 10, 'epsilon': 0.01, 'gamma': 'scale', 'kernel': 'rbf'} if best_model_type != 'SVR' else best_params\n",
    "    models['SVR'] = SVR(**svr_params).fit(X_train_scaled_df, y_train)\n",
    "    \n",
    "    # Best Random Forest model\n",
    "    rf_params = {'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200} if best_model_type != 'RandomForest' else best_params\n",
    "    models['RandomForest'] = RandomForestRegressor(random_state=42, **rf_params).fit(X_train_scaled_df, y_train)\n",
    "    \n",
    "    # Best XGBoost model\n",
    "    xgb_params = {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 5, 'subsample': 0.9} if best_model_type != 'XGBoost' else best_params\n",
    "    models['XGBoost'] = XGBRegressor(random_state=42, **xgb_params).fit(X_train_scaled_df, y_train)\n",
    "    \n",
    "    # Best Lasso model\n",
    "    lasso_params = {'alpha': 0.01} if best_model_type != 'Lasso' else best_params\n",
    "    models['Lasso'] = Lasso(random_state=42, **lasso_params).fit(X_train_scaled_df, y_train)\n",
    "    \n",
    "    # 12. Create ensemble\n",
    "    ensemble = create_ensemble(X_train_scaled_df, y_train, models)\n",
    "    \n",
    "    # 13. Generate training predictions for visualization\n",
    "    train_predictions = {name: model.predict(X_train_scaled_df) for name, model in models.items()}\n",
    "    train_predictions['Ensemble'] = ensemble['predict'](X_train_scaled_df)\n",
    "    \n",
    "    # 14. Visualize model results\n",
    "    model_visualizations = visualize_model_results(model_results, y_train, models, selected_features)\n",
    "    \n",
    "    # 15. Make predictions on test data\n",
    "    test_predictions_df = make_predictions(models, X_test_scaled_df, ensemble)\n",
    "    \n",
    "    # 16. Create submission file with ensemble predictions\n",
    "    submission['T80'] = ensemble['predict'](X_test_scaled_df)\n",
    "    submission.to_csv('ensemble_submission.csv', index=False)\n",
    "    \n",
    "    # 17. Train and save final model\n",
    "    final_model = train_final_model(X_train_scaled_df, y_train, best_model_type, best_params)\n",
    "    \n",
    "    # 18. Save project components\n",
    "    feature_engineering_info = {\n",
    "        'key_features': ['TDOS4.0', 'NumHeteroatoms', 'Mass'],\n",
    "        'rdkit_features': True,\n",
    "        'cleaning_applied': True\n",
    "    }\n",
    "    project_components = save_project_components(scaler, selected_features, feature_engineering_info, best_model_type, best_params)\n",
    "    \n",
    "    print(\"\\n==== Pipeline Complete ====\")\n",
    "    print(f\"Best model: {best_model_type}\")\n",
    "    print(f\"Best RMSE: {best_score:.4f}\")\n",
    "    print(f\"Selected features: {len(selected_features)}\")\n",
    "    print(\"Submission file saved as 'ensemble_submission.csv'\")\n",
    "    \n",
    "    return {\n",
    "        'train_data': df_train_enhanced,\n",
    "        'test_data': df_test_enhanced,\n",
    "        'selected_features': selected_features,\n",
    "        'scaler': scaler,\n",
    "        'models': models,\n",
    "        'ensemble': ensemble,\n",
    "        'best_model': final_model,\n",
    "        'best_model_type': best_model_type,\n",
    "        'best_params': best_params,\n",
    "        'submission': submission,\n",
    "        'visualizations': {**visualizations, **model_visualizations}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acacd02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d3a5ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f08c8d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rec_sys_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
